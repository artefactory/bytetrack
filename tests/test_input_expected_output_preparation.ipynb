{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary: Integration Test for Bytetrack and Yolo model\n",
    "\n",
    "This is the notebook for generation the source ground of truth and input that will be reference for integration tests \n",
    "\n",
    "1. **Preparation of expected output for the test**: \n",
    "   The 2 videos are decomposed into frames, into which we apply the object detection and tracking. We store the frames with tracking under **expected_output** folder. This will be the source of ground truth for the test.\n",
    "\n",
    "2. **Preparation of inputs for the test**: \n",
    "   The 2 videos are decomposed into frames, into which we apply only the object detection. We store the frames with detection under **test_input** folder. This will be the input of bytetracker update method for the test in _test_model_regression.py_ script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# YOLO and video packages \n",
    "from ultralytics import YOLO\n",
    "from bytetracker import BYTETracker\n",
    "from bytetracker.basetrack import BaseTrack\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_results_to_bytetrack_format(detections):\n",
    "    \"\"\"Transforms YOLO detections into the bytetrack format.\n",
    "\n",
    "    Args:\n",
    "        detections: A list of YOLO detections.\n",
    "\n",
    "    Returns:\n",
    "        A list of bytetrack detections.\n",
    "    \"\"\"\n",
    "    boxes = detections.numpy().boxes.xyxyn\n",
    "    scores = detections.numpy().boxes.conf\n",
    "    classes = detections.numpy().boxes.cls\n",
    "    return np.stack(\n",
    "        [\n",
    "            boxes[:, 0],\n",
    "            boxes[:, 1],\n",
    "            boxes[:, 2],\n",
    "            boxes[:, 3],\n",
    "            scores,\n",
    "            classes,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reading 2 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the video\n",
    "VIDEO_PATH_1 = 'videos/traffic.mp4'\n",
    "VIDEO_PATH_2 = 'videos/fruit.mp4'\n",
    "!if [ ! -f $VIDEO_PATH_1 ]; then mkdir -p videos && wget https://storage.googleapis.com/bytetrack-data-public/traffic.mp4 -O $VIDEO_PATH_1; fi\n",
    "!if [ ! -f $VIDEO_PATH_2 ]; then mkdir -p videos && wget https://storage.googleapis.com/bytetrack-data-public/fruit.mp4 -O $VIDEO_PATH_2; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(VIDEO_PATH_1, width=800,embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(VIDEO_PATH_2, width=800,embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yolo model and bytetrack preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will track only car \n",
    "CAR_CLASS_ID = 2\n",
    "FRUIT_CLASS_ID = 49\n",
    "PEOPLE_CLASS_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS = \"yolov8m.pt\"\n",
    "model = YOLO(MODEL_WEIGHTS, task=\"detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = BYTETracker(track_thresh= 0.15, track_buffer = 3, match_thresh = 0.85, frame_rate= 12)\n",
    "BaseTrack._count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO 1 to frames\n",
    "!mkdir -p frames && ffmpeg -i $VIDEO_PATH_1 -vf fps=12 frames/video_1_%d.png -hide_banner -loglevel panic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_frames_1 = glob.glob(\"frames/video_1_*.png\")\n",
    "available_frames_1 = sorted(available_frames_1, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating the first video expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracked_objects_1  = []\n",
    "for frame_id, image_filename in enumerate(available_frames_1):\n",
    "    img = cv2.imread(image_filename)\n",
    "    detections = model.predict(img, classes=[CAR_CLASS_ID], conf=0.15, verbose=False)[0]\n",
    "    detections_bytetrack_format = yolo_results_to_bytetrack_format(detections)\n",
    "    tracked_objects = tracker.update(detections_bytetrack_format, frame_id)\n",
    "    if len(tracked_objects) > 0:\n",
    "        tracked_objects = np.insert(tracked_objects, 0, frame_id, axis=1)\n",
    "        all_tracked_objects_1.append(tracked_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"expected_output/objects_detected_and_tracked_video1.txt\"\n",
    "\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    for array in all_tracked_objects_1:\n",
    "        for row in array:\n",
    "            line = \" \".join(map(str, row)) + \"\\n\"\n",
    "            file.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating the first video detection object frames, as input for integration test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections_by_frame_1 = []\n",
    "\n",
    "for frame_id, image_filename in enumerate(available_frames_1):\n",
    "    img = cv2.imread(image_filename)\n",
    "    detections = model.predict(img, classes=[CAR_CLASS_ID], conf=0.15, verbose=False)[0]\n",
    "    detections_bytetrack_format = yolo_results_to_bytetrack_format(detections)\n",
    "    all_detections_by_frame_1.append((frame_id, detections_bytetrack_format))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_file_path = \"test_input/objects_detected_video1.txt\"\n",
    "\n",
    "with open(detections_file_path, \"w\") as file:\n",
    "    for frame_id, detections_bytetrack_format in all_detections_by_frame_1:\n",
    "        for detection in detections_bytetrack_format:\n",
    "            line = \" \".join(map(str, [frame_id] + list(detection))) + \"\\n\"\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO 2 to frames\n",
    "!mkdir -p frames && ffmpeg -i $VIDEO_PATH_2 -vf fps=12 frames/video_2_%d.png -hide_banner -loglevel panic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_frames_2 = glob.glob(\"frames/video_2_*.png\")\n",
    "available_frames_2 = sorted(available_frames_2, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating the second video expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracked_objects_2  = []\n",
    "for frame_id, image_filename in enumerate(available_frames_2):\n",
    "    img = cv2.imread(image_filename)\n",
    "    detections = model.predict(img, classes=[FRUIT_CLASS_ID], conf=0.15, verbose=False)[0]\n",
    "    detections_bytetrack_format = yolo_results_to_bytetrack_format(detections)\n",
    "    tracked_objects = tracker.update(detections_bytetrack_format, frame_id)\n",
    "    if len(tracked_objects) > 0:\n",
    "        tracked_objects = np.insert(tracked_objects, 0, frame_id, axis=1)\n",
    "        all_tracked_objects_2.append(tracked_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"expected_output/objects_detected_and_tracked_video2.txt\"\n",
    "\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    for array in all_tracked_objects_2:\n",
    "        for row in array:\n",
    "            line = \" \".join(map(str, row)) + \"\\n\"\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating the second video detection object frames, as input for integration test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections_by_frame_2 = []\n",
    "\n",
    "for frame_id, image_filename in enumerate(available_frames_2):\n",
    "    img = cv2.imread(image_filename)\n",
    "    detections = model.predict(img, classes=[FRUIT_CLASS_ID], conf=0.15, verbose=False)[0]\n",
    "    detections_bytetrack_format = yolo_results_to_bytetrack_format(detections)\n",
    "    if len(detections_bytetrack_format) > 0:\n",
    "        all_detections_by_frame_2.append((frame_id, detections_bytetrack_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_file_path = \"test_input/objects_detected_video2.txt\"\n",
    "\n",
    "with open(detections_file_path, \"w\") as file:\n",
    "    for frame_id, detections_bytetrack_format in all_detections_by_frame_2:\n",
    "        for detection in detections_bytetrack_format:\n",
    "            line = \" \".join(map(str, [frame_id] + list(detection))) + \"\\n\"\n",
    "            file.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
